{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.core import TruSession\n",
    "from trulens.core import Feedback\n",
    "from trulens.core.schema.select import Select\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "from trulens.apps.custom import instrument\n",
    "from trulens.dashboard import run_dashboard\n",
    "from utils.chunk_scorer import score_chunk\n",
    "\n",
    "\n",
    "\n",
    "class retriever_evaluator:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,name, ground_truth, rag_app , reset_db = False):\n",
    "        self.name = name\n",
    "        self.rag_app = rag_app\n",
    "        self.session = self._init_db(reset_db)\n",
    "        self.ground_truth = self._init_ground_truth(ground_truth) \n",
    "        self.feedback = self._feedback_init()\n",
    "        self.tru_app = self._init_app()\n",
    "\n",
    "### Move the addition of the scores  to prepare ground truth \n",
    "    def _init_ground_truth(self,ground_truth):\n",
    "        for i in range(len(ground_truth[\"query\"])):\n",
    "            queries =  ground_truth[\"query\"]\n",
    "            expected_responses =  ground_truth[\"expected_response\"]\n",
    "            expected_chunks = ground_truth[\"expected_chunks\"]\n",
    "            expected_chunks[i] = [{\"text\":expected_chunk, \"title\":expected_chunk, \"expected_score\":score_chunk(expected_chunk,expected_responses[i])} for expected_chunk in expected_chunks[i] ]\n",
    "            df={\"query\":[queries[i]],\"expected_response\":[expected_responses[i]],\"expected_chunks\":[expected_chunks[i]],\"query_id\":[str(i+1)]}\n",
    "            self.session.add_ground_truth_to_dataset(\n",
    "                dataset_name=\"groundtruth\",\n",
    "                ground_truth_df=pd.DataFrame(df),\n",
    "                dataset_metadata={\"domain\": \"Data from Ministry of Health UAE\"},)\n",
    "\n",
    "        \n",
    "        return self.session.get_ground_truth(\"groundtruth\")\n",
    "\n",
    "    def _init_db(self, reset_db):\n",
    "        session = TruSession()\n",
    "        session.reset_database() if reset_db else None\n",
    "\n",
    "        return session\n",
    "    \n",
    "    def _feedback_init(self):\n",
    "        arg_query_selector = (\n",
    "            Select.RecordCalls.retrieve_and_generate.args.query\n",
    "        )  # 1st argument of retrieve_and_generate function\n",
    "        arg_retrieval_k_selector = (\n",
    "            Select.RecordCalls.retrieve_and_generate.args.k\n",
    "        )  # 2nd argument of retrieve_and_generate function\n",
    "\n",
    "        arg_completion_str_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            0\n",
    "        ]  # 1st returned value from retrieve_and_generate function\n",
    "        arg_retrieved_context_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            1\n",
    "        ]  # 2nd returned value from retrieve_and_generate function\n",
    "        arg_relevance_scores_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            2\n",
    "        ]  # last returned value from retrieve_and_generate function\n",
    "\n",
    "        f_ir_hit_rate = (\n",
    "            Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).ir_hit_rate,\n",
    "                name=\"IR hit rate\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "\n",
    "        f_ndcg_at_k = (\n",
    "            Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).ndcg_at_k,\n",
    "                name=\"NDCG@k\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_relevance_scores_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "\n",
    "\n",
    "        f_recall_at_k = (\n",
    "                Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).recall_at_k,\n",
    "                name=\"Recall@k\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_relevance_scores_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "        f_groundtruth_answer = (\n",
    "            Feedback(\n",
    "            GroundTruthAgreement(self.ground_truth).agreement_measure,\n",
    "            name=\"Ground Truth answer (semantic similarity)\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_completion_str_selector))\n",
    "        return [f_ir_hit_rate, f_ndcg_at_k, f_recall_at_k, f_groundtruth_answer]\n",
    "\n",
    "    def _init_app(self):\n",
    "\n",
    "        tru_app = TruCustomApp(\n",
    "            self.rag_app,\n",
    "            app_name=self.name,\n",
    "            feedbacks=self.feedback,\n",
    "            )\n",
    "        return tru_app\n",
    "    def run(self ):\n",
    "        queries = self.ground_truth[\"query\"]\n",
    "        for i,query in enumerate(queries):\n",
    "            with self.tru_app as recording:\n",
    "                self.rag_app.retrieve_and_generate(query,10)\n",
    "    def leaderboard(self):\n",
    "        self.session.get_leaderboard(app_ids=[self.tru_app.app_id])\n",
    "\n",
    "\n",
    "\n",
    "class rag_app:\n",
    "    def __init__(self, retriever, generator, expected_responses,queries):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        self.expected_responses = expected_responses\n",
    "        self.queries = queries\n",
    "    \n",
    "    def _get_scores(self,chunks,expected_response):\n",
    "        chunks = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "        return [ score_chunk( chunk , expected_response)  for chunk in chunks]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @instrument\n",
    "    def retrieve_and_generate(self, query, k,):\n",
    "        chunks = self.retriever.get_Chunks(query)\n",
    "        chunks_dict = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "        response = self.generator.generate(query, chunks_dict)\n",
    "        i = self.queries.index(query)\n",
    "        expected_response = self.expected_responses[i]\n",
    "        scores = self._get_scores(chunks,expected_response)\n",
    "\n",
    "        return response, chunks_dict, scores\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "ðŸ¦‘ Initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
      "Updating app_id in records table: 0it [00:00, ?it/s]\n",
      "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In IR hit rate, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "âœ… In IR hit rate, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "âœ… In IR hit rate, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "âœ… In NDCG@k, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "âœ… In NDCG@k, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "âœ… In NDCG@k, input relevance_scores will be set to __record__.app.retrieve_and_generate.rets[2] .\n",
      "âœ… In NDCG@k, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "âœ… In Recall@k, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "âœ… In Recall@k, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "âœ… In Recall@k, input relevance_scores will be set to __record__.app.retrieve_and_generate.rets[2] .\n",
      "âœ… In Recall@k, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "âœ… In Ground Truth answer (semantic similarity), input prompt will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "âœ… In Ground Truth answer (semantic similarity), input response will be set to __record__.app.retrieve_and_generate.rets[0] .\n"
     ]
    }
   ],
   "source": [
    "from cohere_ret.cohere_ret import cohere_retriever\n",
    "from cohere_ret.generator import cohere_generator\n",
    "from gemini.retrieve import gemini_retriever\n",
    "from openai_class.retriever import openai_retriever\n",
    "from openai_class.generator import openai_generator\n",
    "from voyageai_ret.retrieve import voyage_retriever\n",
    "from gemini.generator import gemini_generator\n",
    "from utils.prepare_ground_truth import LatestGroundTruthCSV\n",
    "from evaluator.ret_eval import rag_app, retriever_evaluator\n",
    "from utils.chunk_scorer import score_chunk\n",
    "csv_filepath = 'GroundTruths_Dataset - Sheet1.csv'\n",
    "json_filepath = 'url_chunk_mapping_1000_v2.0.json'\n",
    "\n",
    "processor = LatestGroundTruthCSV(csv_filepath, json_filepath)\n",
    "ground_truth = processor.get_latest_ground_truth()\n",
    "\n",
    "\n",
    "\n",
    "ret = openai_retriever()\n",
    "gen = gemini_generator()\n",
    "rag_app = rag_app(ret, gen,ground_truth[\"expected_response\"],ground_truth[\"query\"])\n",
    "\n",
    "#eval-{Retriever}-{generator}-{chunksize}\n",
    "ret_eval = retriever_evaluator(name=\"eval_openai_gemini-1000\",ground_truth=ground_truth,rag_app=rag_app, reset_db=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved and evaluated 4.3478260869565215% \"How do I register for controlled or semi-controlled drugs custody?\"\n",
      "retrieved and evaluated 8.695652173913043% \"What are the requirements for renewing the registration of a conventional pharmaceutical product?\"\n",
      "retrieved and evaluated 13.043478260869565% \"How do I appeal a decision made by the Medical Licensing Committee?\"\n",
      "retrieved and evaluated 17.391304347826086% \"What is the process for obtaining a certificate of amendment for registered pharmaceutical products?\"\n",
      "retrieved and evaluated 21.73913043478261% \"How can I get a product classified?\"\n",
      "retrieved and evaluated 26.08695652173913% \"What are the steps to re-license a pharmaceutical facility?\"\n",
      "retrieved and evaluated 30.434782608695652% \"How can I renew my license as a nurse or medical professional?\"\n",
      "retrieved and evaluated 34.78260869565217% \"What's the process for getting a permit to import medical equipment?\"\n",
      "retrieved and evaluated 39.130434782608695% \"How can I renew my health facility license?\"\n",
      "retrieved and evaluated 43.47826086956522% \"What are the steps to register a change in a doctor's professional title?\"\n",
      "retrieved and evaluated 47.82608695652174% \"How do I re-license a health facility after cancellation or suspension?\"\n",
      "retrieved and evaluated 52.17391304347826% \"How do I change the technical director of my private medical facility?\"\n",
      "retrieved and evaluated 56.52173913043478% \"What's the process for re-licensing nurses and medical professionals?\"\n",
      "retrieved and evaluated 60.869565217391305% \"How can I request a list of licensed pharmaceutical facilities in the UAE?\"\n",
      "retrieved and evaluated 65.21739130434783% \"How do I renew my registration certificate to practice nursing or midwifery?\"\n",
      "retrieved and evaluated 69.56521739130434% \"What are the requirement documents for the good standing certificate of medical staff in the sector the is fee-exempt for renewal staff licenses?\"\n",
      "retrieved and evaluated 73.91304347826087% \" I have a medical equipment that is manufactured from animal-based products, What is the condition to renew the license? \"\n",
      "retrieved and evaluated 78.26086956521739% \"What are the fees to renew the document I get to open a clinic in the UAE? \"\n",
      "retrieved and evaluated 82.6086956521739% \"What are the required documents to apply for the approval of the service that enables employees to apply and approve their applications but for private entity employees? \"\n",
      "retrieved and evaluated 86.95652173913044% \"Do healthcare facilities need to fulfill specific requirements regarding elevator installations and what medical staff arrangements are required for operations?\"\n",
      "retrieved and evaluated 91.30434782608695% \"Can pharmaceutical companies export narcotic drugs and what are the validity requirements for such permits?\"\n",
      "retrieved and evaluated 95.65217391304348% \"Are there specific requirements for medical professionals over 60 years old and what documentation is needed for their continued practice?\"\n",
      "retrieved and evaluated 100.0% \"What restrictions apply to medical advertising on websites and social media, and how does the licensing differ between platforms?\"\n"
     ]
    }
   ],
   "source": [
    "ret_eval.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Ground Truth answer (semantic similarity)</th>\n",
       "      <th>IR hit rate</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_openai_gemini-1000</th>\n",
       "      <th>base</th>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367902</td>\n",
       "      <td>4.877454</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Ground Truth answer (semantic similarity)  \\\n",
       "app_name                app_version                                              \n",
       "eval_openai_gemini-1000 base                                          0.759091   \n",
       "\n",
       "                                     IR hit rate  NDCG@k  Recall@k   latency  \\\n",
       "app_name                app_version                                            \n",
       "eval_openai_gemini-1000 base            0.956522     1.0  0.367902  4.877454   \n",
       "\n",
       "                                     total_cost  \n",
       "app_name                app_version              \n",
       "eval_openai_gemini-1000 base                0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_eval.session.get_leaderboard(app_ids=[ret_eval.tru_app.app_id,\"app_hash_fb4995701b8d40691d749df9464d2b3b\",\"app_hash_5900e9c221404923bfe3b4748a08c898\",\"app_hash_43aa1da8d29ea67fe751b00bcbc81856\",\"app_hash_44ad3fc2973d346fbba7ffff72375bee\",\"app_hash_7fc04a06e84483e4843c2d36e2441d58\",\"app_hash_06489213760ec9904f851ebc0e4a6681\",\"app_hash_85fca6bb738e30758f6f96559de669d6\",\"app_hash_7bb306d11afd98b36c726ea4147bcea3\",\"app_hash_af2d2f2c512504fcede40b939672677d\",\"app_hash_c8146a11589aeb100704fed81a3b8ec7\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app_hash_1a0ce2a495f8f8278d9ed0d117e7ec87'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_eval.tru_app.app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
