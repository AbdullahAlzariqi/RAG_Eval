{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.core import TruSession\n",
    "from trulens.core import Feedback\n",
    "from trulens.core.schema.select import Select\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "from trulens.apps.custom import instrument\n",
    "from trulens.dashboard import run_dashboard\n",
    "from utils.chunk_scorer import score_chunk\n",
    "\n",
    "\n",
    "\n",
    "class retriever_evaluator:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,name, ground_truth, rag_app , reset_db = False):\n",
    "        self.name = name\n",
    "        self.rag_app = rag_app\n",
    "        self.session = self._init_db(reset_db)\n",
    "        self.ground_truth = self._init_ground_truth(ground_truth) \n",
    "        self.feedback = self._feedback_init()\n",
    "        self.tru_app = self._init_app()\n",
    "\n",
    "### Move the addition of the scores  to prepare ground truth \n",
    "    def _init_ground_truth(self,ground_truth):\n",
    "        for i in range(len(ground_truth[\"query\"])):\n",
    "            queries =  ground_truth[\"query\"]\n",
    "            expected_responses =  ground_truth[\"expected_response\"]\n",
    "            expected_chunks = ground_truth[\"expected_chunks\"]\n",
    "            expected_chunks[i] = [{\"text\":expected_chunk, \"title\":expected_chunk, \"expected_score\":score_chunk(expected_chunk,expected_responses[i])} for expected_chunk in expected_chunks[i] ]\n",
    "            df={\"query\":[queries[i]],\"expected_response\":[expected_responses[i]],\"expected_chunks\":[expected_chunks[i]],\"query_id\":[str(i+1)]}\n",
    "            self.session.add_ground_truth_to_dataset(\n",
    "                dataset_name=\"groundtruth\",\n",
    "                ground_truth_df=pd.DataFrame(df),\n",
    "                dataset_metadata={\"domain\": \"Data from Ministry of Health UAE\"},)\n",
    "\n",
    "        print(self.session.get_ground_truth(\"groundtruth\"))\n",
    "        print(type(self.session.get_ground_truth(\"groundtruth\")))\n",
    "        return self.session.get_ground_truth(\"groundtruth\")\n",
    "\n",
    "    def _init_db(self, reset_db):\n",
    "        session = TruSession()\n",
    "        session.reset_database() if reset_db else None\n",
    "\n",
    "        return session\n",
    "    \n",
    "    def _feedback_init(self):\n",
    "        arg_query_selector = (\n",
    "            Select.RecordCalls.retrieve_and_generate.args.query\n",
    "        )  # 1st argument of retrieve_and_generate function\n",
    "        arg_retrieval_k_selector = (\n",
    "            Select.RecordCalls.retrieve_and_generate.args.k\n",
    "        )  # 2nd argument of retrieve_and_generate function\n",
    "\n",
    "        arg_completion_str_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            0\n",
    "        ]  # 1st returned value from retrieve_and_generate function\n",
    "        arg_retrieved_context_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            1\n",
    "        ]  # 2nd returned value from retrieve_and_generate function\n",
    "        arg_relevance_scores_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            2\n",
    "        ]  # last returned value from retrieve_and_generate function\n",
    "\n",
    "        f_ir_hit_rate = (\n",
    "            Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).ir_hit_rate,\n",
    "                name=\"IR hit rate\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "\n",
    "        f_ndcg_at_k = (\n",
    "            Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).ndcg_at_k,\n",
    "                name=\"NDCG@k\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_relevance_scores_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "\n",
    "\n",
    "        f_recall_at_k = (\n",
    "                Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).recall_at_k,\n",
    "                name=\"Recall@k\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_relevance_scores_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "        f_groundtruth_answer = (\n",
    "            Feedback(\n",
    "            GroundTruthAgreement(self.ground_truth).agreement_measure,\n",
    "            name=\"Ground Truth answer (semantic similarity)\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_completion_str_selector))\n",
    "        return [f_ir_hit_rate, f_ndcg_at_k, f_recall_at_k, f_groundtruth_answer]\n",
    "\n",
    "    def _init_app(self):\n",
    "\n",
    "        tru_app = TruCustomApp(\n",
    "            self.rag_app,\n",
    "            app_name=self.name,\n",
    "            feedbacks=self.feedback,\n",
    "            )\n",
    "        return tru_app\n",
    "    def run(self ):\n",
    "        queries = self.ground_truth[\"query\"]\n",
    "        for i,query in enumerate(queries):\n",
    "            with self.tru_app as recording:\n",
    "                self.rag_app.retrieve_and_generate(query,10)\n",
    "    def leaderboard(self):\n",
    "        self.session.get_leaderboard(app_ids=[self.tru_app.app_id])\n",
    "\n",
    "\n",
    "\n",
    "class rag_app:\n",
    "    def __init__(self, retriever, generator, expected_responses,queries):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        self.expected_responses = expected_responses\n",
    "        self.queries = queries\n",
    "    \n",
    "    def _get_scores(self,chunks,expected_response):\n",
    "        chunks = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "        return [ score_chunk( chunk , expected_response)  for chunk in chunks]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @instrument\n",
    "    def retrieve_and_generate(self, query, k,):\n",
    "        chunks = self.retriever.get_Chunks(query)\n",
    "        chunks_dict = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "        response = self.generator.generate(query, chunks_dict)\n",
    "        i = self.queries.index(query)\n",
    "        expected_response = self.expected_responses[i]\n",
    "        scores = self._get_scores(chunks,expected_response)\n",
    "        print(f\"retrieved and evaluated {((i + 1) * 100) / 23}% \\\"{query}\\\"\")\n",
    "\n",
    "        return response, chunks_dict, scores\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_ids=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cohere_ret.cohere_ret import cohere_retriever\n",
    "# from gemini.retrieve import gemini_retriever\n",
    "# from openai_class.retriever import openai_retriever\n",
    "from openai_class.generator import openai_generator\n",
    "# from voyageai_ret.retrieve import voyage_retriever\n",
    "# from gemini.generator import gemini_generator\n",
    "from utils.prepare_ground_truth import LatestGroundTruthCSV\n",
    "from evaluator.ret_eval import rag_app, retriever_evaluator\n",
    "from utils.chunk_scorer import score_chunk\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "with open(r'data\\data.json', 'r') as f:\n",
    "   ground_truth = json.loads(f.read())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ret = cohere_retriever()\n",
    "gen = openai_generator()\n",
    "rag_app = rag_app(ret, gen,ground_truth[\"expected_response\"],ground_truth[\"query\"])\n",
    "\n",
    "#eval_{Retriever}_{generator}_{chunksize}_@{k}  eval_gemini_openai_528_@1\n",
    "ret_eval = retriever_evaluator(name=\"eval_cohere_openai_528_@3\",ground_truth=ground_truth, rag_app=rag_app, reset_db=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ret_eval.run(k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_ids.append(ret_eval.tru_app.app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_eval.session.get_leaderboard(app_ids=[ret_eval.tru_app.app_id].extend(apps_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_eval.tru_app.app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from  trulens.dashboard import run_dashboard\n",
    "# run_dashboard()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
