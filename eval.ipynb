{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.core import TruSession\n",
    "from trulens.core import Feedback\n",
    "from trulens.core.schema.select import Select\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "from trulens.apps.custom import instrument\n",
    "from trulens.dashboard import run_dashboard\n",
    "from utils.chunk_scorer import score_chunk\n",
    "\n",
    "\n",
    "\n",
    "class retriever_evaluator:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,name, ground_truth, rag_app , reset_db = False):\n",
    "        self.name = name\n",
    "        self.rag_app = rag_app\n",
    "        self.session = self._init_db(reset_db)\n",
    "        self.ground_truth = self._init_ground_truth(ground_truth) \n",
    "        self.feedback = self._feedback_init()\n",
    "        self.tru_app = self._init_app()\n",
    "\n",
    "### Move the addition of the scores  to prepare ground truth \n",
    "    def _init_ground_truth(self,ground_truth):\n",
    "        for i in range(len(ground_truth[\"query\"])):\n",
    "            queries =  ground_truth[\"query\"]\n",
    "            expected_responses =  ground_truth[\"expected_response\"]\n",
    "            expected_chunks = ground_truth[\"expected_chunks\"]\n",
    "            expected_chunks[i] = [{\"text\":expected_chunk, \"title\":expected_chunk, \"expected_score\":score_chunk(expected_chunk,expected_responses[i])} for expected_chunk in expected_chunks[i] ]\n",
    "            df={\"query\":[queries[i]],\"expected_response\":[expected_responses[i]],\"expected_chunks\":[expected_chunks[i]],\"query_id\":[str(i+1)]}\n",
    "            self.session.add_ground_truth_to_dataset(\n",
    "                dataset_name=\"groundtruth\",\n",
    "                ground_truth_df=pd.DataFrame(df),\n",
    "                dataset_metadata={\"domain\": \"Data from Ministry of Health UAE\"},)\n",
    "\n",
    "        \n",
    "        return self.session.get_ground_truth(\"groundtruth\")\n",
    "\n",
    "    def _init_db(self, reset_db):\n",
    "        session = TruSession()\n",
    "        session.reset_database() if reset_db else None\n",
    "\n",
    "        return session\n",
    "    \n",
    "    def _feedback_init(self):\n",
    "        arg_query_selector = (\n",
    "            Select.RecordCalls.retrieve_and_generate.args.query\n",
    "        )  # 1st argument of retrieve_and_generate function\n",
    "        arg_retrieval_k_selector = (\n",
    "            Select.RecordCalls.retrieve_and_generate.args.k\n",
    "        )  # 2nd argument of retrieve_and_generate function\n",
    "\n",
    "        arg_completion_str_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            0\n",
    "        ]  # 1st returned value from retrieve_and_generate function\n",
    "        arg_retrieved_context_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            1\n",
    "        ]  # 2nd returned value from retrieve_and_generate function\n",
    "        arg_relevance_scores_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            2\n",
    "        ]  # last returned value from retrieve_and_generate function\n",
    "\n",
    "        f_ir_hit_rate = (\n",
    "            Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).ir_hit_rate,\n",
    "                name=\"IR hit rate\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "\n",
    "        f_ndcg_at_k = (\n",
    "            Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).ndcg_at_k,\n",
    "                name=\"NDCG@k\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_relevance_scores_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "\n",
    "\n",
    "        f_recall_at_k = (\n",
    "                Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).recall_at_k,\n",
    "                name=\"Recall@k\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_relevance_scores_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "        f_groundtruth_answer = (\n",
    "            Feedback(\n",
    "            GroundTruthAgreement(self.ground_truth).agreement_measure,\n",
    "            name=\"Ground Truth answer (semantic similarity)\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_completion_str_selector))\n",
    "        return [f_ir_hit_rate, f_ndcg_at_k, f_recall_at_k, f_groundtruth_answer]\n",
    "\n",
    "    def _init_app(self):\n",
    "\n",
    "        tru_app = TruCustomApp(\n",
    "            self.rag_app,\n",
    "            app_name=self.name,\n",
    "            feedbacks=self.feedback,\n",
    "            )\n",
    "        return tru_app\n",
    "    def run(self ):\n",
    "        queries = self.ground_truth[\"query\"]\n",
    "        for i,query in enumerate(queries):\n",
    "            with self.tru_app as recording:\n",
    "                self.rag_app.retrieve_and_generate(query,10)\n",
    "    def leaderboard(self):\n",
    "        self.session.get_leaderboard(app_ids=[self.tru_app.app_id])\n",
    "\n",
    "\n",
    "\n",
    "class rag_app:\n",
    "    def __init__(self, retriever, generator, expected_responses,queries):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        self.expected_responses = expected_responses\n",
    "        self.queries = queries\n",
    "    \n",
    "    def _get_scores(self,chunks,expected_response):\n",
    "        chunks = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "        return [ score_chunk( chunk , expected_response)  for chunk in chunks]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @instrument\n",
    "    def retrieve_and_generate(self, query, k,):\n",
    "        chunks = self.retriever.get_Chunks(query)\n",
    "        chunks_dict = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "        response = self.generator.generate(query, chunks_dict)\n",
    "        i = self.queries.index(query)\n",
    "        expected_response = self.expected_responses[i]\n",
    "        scores = self._get_scores(chunks,expected_response)\n",
    "\n",
    "        return response, chunks_dict, scores\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      ground_truth_id  \\\n",
      "0   ground_truth_hash_61d89bb73141b372c1d35ab29c0e...   \n",
      "1   ground_truth_hash_86f6b8551102b6d18293be593e9b...   \n",
      "2   ground_truth_hash_12c36d6a2fbb73e671cfe814b3db...   \n",
      "3   ground_truth_hash_e5a167d713d7858ed9e8c6e74f5b...   \n",
      "4   ground_truth_hash_0dad6c914ddb1d5248355c5499bb...   \n",
      "5   ground_truth_hash_e61b5b2f36280abefc00df7554c5...   \n",
      "6   ground_truth_hash_8714bd9ce7ee36bf8c1f9541fa3b...   \n",
      "7   ground_truth_hash_d17304ff0c441ec955012a1f4514...   \n",
      "8   ground_truth_hash_a39c5ec013ff6149fe7aafa4d478...   \n",
      "9   ground_truth_hash_2908a3c58d7d02c338e95ebc8008...   \n",
      "10  ground_truth_hash_80b3d599bcf887b87806ac5faeff...   \n",
      "11  ground_truth_hash_716eb0af6b3447690e561184291a...   \n",
      "12  ground_truth_hash_87a6da649ede154f6aa52a82fa1c...   \n",
      "13  ground_truth_hash_a06602996a5dfb64515d882720f4...   \n",
      "14  ground_truth_hash_1e9653e7fed501dfc85dc86d1824...   \n",
      "15  ground_truth_hash_3abec1a0480195159a569c428970...   \n",
      "16  ground_truth_hash_cdc146d32b2463bf5152b88d78a0...   \n",
      "17  ground_truth_hash_aeba0a0b7d541a81d748405c3363...   \n",
      "18  ground_truth_hash_0b00464e395491d59b8efebce761...   \n",
      "\n",
      "                                       dataset_id  \\\n",
      "0   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "1   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "2   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "3   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "4   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "5   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "6   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "7   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "8   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "9   dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "10  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "11  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "12  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "13  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "14  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "15  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "16  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "17  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "18  dataset_hash_c07d91c637324b21a9c17a024e392518   \n",
      "\n",
      "                                                query query_id  \\\n",
      "0   How do I register for controlled or semi-contr...        1   \n",
      "1   What are the requirements for renewing the reg...        2   \n",
      "2   How do I appeal a decision made by the Medical...        3   \n",
      "3   What is the process for obtaining a certificat...        4   \n",
      "4                 How can I get a product classified?        5   \n",
      "5   What are the steps to re-license a pharmaceuti...        6   \n",
      "6   How can I renew my license as a nurse or medic...        7   \n",
      "7   What's the process for getting a permit to imp...        8   \n",
      "8         How can I renew my health facility license?        9   \n",
      "9   What are the steps to register a change in a d...       10   \n",
      "10  How do I re-license a health facility after ca...       11   \n",
      "11  How do I change the technical director of my p...       12   \n",
      "12  What's the process for re-licensing nurses and...       13   \n",
      "13  How can I request a list of licensed pharmaceu...       14   \n",
      "14  How do I renew my registration certificate to ...       15   \n",
      "15  What are the requirement documents for the goo...       16   \n",
      "16   I have a medical equipment that is manufactur...       17   \n",
      "17  What are the fees to renew the document I get ...       18   \n",
      "18  What are the required documents to apply for t...       19   \n",
      "\n",
      "                                    expected_response  \\\n",
      "0   This service enables clinics, pharmacies, and ...   \n",
      "1   To renew the registration, you must be a Marke...   \n",
      "2   You can appeal a decision by submitting an app...   \n",
      "3   You can apply for a certificate of amendment t...   \n",
      "4   This service classifies products based on pres...   \n",
      "5   You can re-license a pharmaceutical facility a...   \n",
      "6   Medical facilities can renew licenses for nurs...   \n",
      "7   Local agents with a valid medical store licens...   \n",
      "8   Health facilities can renew their licenses onl...   \n",
      "9   Medical facilities can apply to register chang...   \n",
      "10  Apply online through your MoHAP account, payin...   \n",
      "11  Medical facilities can submit applications to ...   \n",
      "12  Medical facilities apply for re-licensing thro...   \n",
      "13  Apply through your MoHAP account, provide the ...   \n",
      "14  Apply online using UAE PASS. Submit informatio...   \n",
      "15  A letter of experience from the Department of ...   \n",
      "16  Marketing Authorization Holder companies must ...   \n",
      "17  \\r\\nLicense application fee: AED 100\\r\\n\\r\\nFi...   \n",
      "18  Required Documents\\r\\nâ€¢ Sick leave of 5 days...   \n",
      "\n",
      "                                      expected_chunks meta  \n",
      "0   [{'text': 'Register for Controlled or Semi-Con...   {}  \n",
      "1   [{'text': 'Renewal of Registration of a Conven...   {}  \n",
      "2   [{'text': 'Appeal Against Medical Licensing Co...   {}  \n",
      "3   [{'text': 'Issue of a Certificate of Amendment...   {}  \n",
      "4   [{'text': 'Classification of a productStart Se...   {}  \n",
      "5   [{'text': 'Re-license (Re-register) a Pharmace...   {}  \n",
      "6   [{'text': 'Renewal of Licenses for Nursing and...   {}  \n",
      "7   [{'text': 'Issue of Permit to Import Medical E...   {}  \n",
      "8   [{'text': 'Renew a Health Facility LicenseStar...   {}  \n",
      "9   [{'text': 'Registration of a Change to the Pro...   {}  \n",
      "10  [{'text': 'Re-license (Re-register) a Health F...   {}  \n",
      "11  [{'text': 'Changing the Technical Director of ...   {}  \n",
      "12  [{'text': 'Re-licensing of Licenses for Nurses...   {}  \n",
      "13  [{'text': 'List of Licensed Pharmaceutical Fac...   {}  \n",
      "14  [{'text': 'Renewal of Registration Certificate...   {}  \n",
      "15  [{'text': 'Issue a Good Standing Certificate f...   {}  \n",
      "16  [{'text': 'Issue of a Certificate of Free Sale...   {}  \n",
      "17  [{'text': 'Renew a Health Facility LicenseStar...   {}  \n",
      "18  [{'text': 'Menuعربي LoginHomeServicesIndividua...   {}  \n",
      "✅ In IR hit rate, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "✅ In IR hit rate, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "✅ In IR hit rate, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "✅ In NDCG@k, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "✅ In NDCG@k, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "✅ In NDCG@k, input relevance_scores will be set to __record__.app.retrieve_and_generate.rets[2] .\n",
      "✅ In NDCG@k, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "✅ In Recall@k, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "✅ In Recall@k, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "✅ In Recall@k, input relevance_scores will be set to __record__.app.retrieve_and_generate.rets[2] .\n",
      "✅ In Recall@k, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "✅ In Ground Truth answer (semantic similarity), input prompt will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "✅ In Ground Truth answer (semantic similarity), input response will be set to __record__.app.retrieve_and_generate.rets[0] .\n"
     ]
    }
   ],
   "source": [
    "from cohere_ret.cohere_ret import cohere_retriever\n",
    "from cohere_ret.generator import cohere_generator\n",
    "from gemini.retrieve import gemini_retriever\n",
    "from openai_class.retriever import openai_retriever\n",
    "from voyageai_ret.retrieve import voyage_retriever\n",
    "from gemini.generator import gemini_generator\n",
    "from utils.prepare_ground_truth import LatestGroundTruthCSV\n",
    "from evaluator.ret_eval import rag_app, retriever_evaluator\n",
    "from utils.chunk_scorer import score_chunk\n",
    "csv_filepath = 'GroundTruths_Dataset - Sheet1.csv'\n",
    "json_filepath = 'URL-chunk_map.json'\n",
    "\n",
    "processor = LatestGroundTruthCSV(csv_filepath, json_filepath)\n",
    "ground_truth = processor.get_latest_ground_truth()\n",
    "\n",
    "\n",
    "\n",
    "ret = cohere_retriever()\n",
    "gen = cohere_generator()\n",
    "rag_app = rag_app(ret, gen,ground_truth[\"expected_response\"],ground_truth[\"query\"])\n",
    "\n",
    "\n",
    "ret_eval = retriever_evaluator(name=\"eval_cohere2_cohere\",ground_truth=ground_truth,rag_app=rag_app)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret_eval.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Ground Truth answer (semantic similarity)</th>\n",
       "      <th>IR hit rate</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_cohere_cohere</th>\n",
       "      <th>base</th>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.274351</td>\n",
       "      <td>5.443205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_cohere2_cohere</th>\n",
       "      <th>base</th>\n",
       "      <td>0.721053</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358413</td>\n",
       "      <td>5.165441</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_openai_cohere</th>\n",
       "      <th>base</th>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.274351</td>\n",
       "      <td>5.977364</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_gemini_cohere</th>\n",
       "      <th>base</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.346678</td>\n",
       "      <td>5.720447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_voyage_cohere</th>\n",
       "      <th>base</th>\n",
       "      <td>0.584211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269593</td>\n",
       "      <td>5.577336</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Ground Truth answer (semantic similarity)  \\\n",
       "app_name            app_version                                              \n",
       "eval_cohere_cohere  base                                          0.752632   \n",
       "eval_cohere2_cohere base                                          0.721053   \n",
       "eval_openai_cohere  base                                          0.710526   \n",
       "eval_gemini_cohere  base                                          0.684211   \n",
       "eval_voyage_cohere  base                                          0.584211   \n",
       "\n",
       "                                 IR hit rate  NDCG@k  Recall@k   latency  \\\n",
       "app_name            app_version                                            \n",
       "eval_cohere_cohere  base            0.947368     1.0  0.274351  5.443205   \n",
       "eval_cohere2_cohere base            0.894737     1.0  0.358413  5.165441   \n",
       "eval_openai_cohere  base            0.947368     1.0  0.274351  5.977364   \n",
       "eval_gemini_cohere  base            0.947368     1.0  0.346678  5.720447   \n",
       "eval_voyage_cohere  base            1.000000     1.0  0.269593  5.577336   \n",
       "\n",
       "                                 total_cost  \n",
       "app_name            app_version              \n",
       "eval_cohere_cohere  base                0.0  \n",
       "eval_cohere2_cohere base                0.0  \n",
       "eval_openai_cohere  base                0.0  \n",
       "eval_gemini_cohere  base                0.0  \n",
       "eval_voyage_cohere  base                0.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_eval.session.get_leaderboard(app_ids=[ret_eval.tru_app.app_id,\"app_hash_ff8b170f597ed0688129aafc72a66596\",\"app_hash_54d3e06551724bf80729fddacf6fb2ad\",\"app_hash_3149e752fa175d4feff2bca54fa3e414\",\"app_hash_fb9ccd4df04ceb87e7e13ff8ab4a7d87\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app_hash_4addebb53dead30189a4299b3f47c63a'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_eval.tru_app.app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
