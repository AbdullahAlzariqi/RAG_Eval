{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.core import TruSession\n",
    "from trulens.core import Feedback\n",
    "from trulens.core.schema.select import Select\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "from trulens.apps.custom import instrument\n",
    "from trulens.dashboard import run_dashboard\n",
    "from utils.chunk_scorer import score_chunk\n",
    "\n",
    "\n",
    "\n",
    "class retriever_evaluator:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,name, ground_truth, rag_app , reset_db = False):\n",
    "        self.name = name\n",
    "        self.rag_app = rag_app\n",
    "        self.session = self._init_db(reset_db)\n",
    "        self.ground_truth = self._init_ground_truth(ground_truth) \n",
    "        self.feedback = self._feedback_init()\n",
    "        self.tru_app = self._init_app()\n",
    "\n",
    "### Move the addition of the scores  to prepare ground truth \n",
    "    def _init_ground_truth(self,ground_truth):\n",
    "        for i in range(len(ground_truth[\"query\"])):\n",
    "            queries =  ground_truth[\"query\"]\n",
    "            expected_responses =  ground_truth[\"expected_response\"]\n",
    "            expected_chunks = ground_truth[\"expected_chunks\"]\n",
    "            expected_chunks[i] = [{\"text\":expected_chunk, \"title\":expected_chunk, \"expected_score\":score_chunk(expected_chunk,expected_responses[i])} for expected_chunk in expected_chunks[i] ]\n",
    "            df={\"query\":[queries[i]],\"expected_response\":[expected_responses[i]],\"expected_chunks\":[expected_chunks[i]],\"query_id\":[str(i+1)]}\n",
    "            self.session.add_ground_truth_to_dataset(\n",
    "                dataset_name=\"groundtruth\",\n",
    "                ground_truth_df=pd.DataFrame(df),\n",
    "                dataset_metadata={\"domain\": \"Data from Ministry of Health UAE\"},)\n",
    "\n",
    "        \n",
    "        return self.session.get_ground_truth(\"groundtruth\")\n",
    "\n",
    "    def _init_db(self, reset_db):\n",
    "        session = TruSession()\n",
    "        session.reset_database() if reset_db else None\n",
    "\n",
    "        return session\n",
    "    \n",
    "    def _feedback_init(self):\n",
    "        arg_query_selector = (\n",
    "            Select.RecordCalls.retrieve_and_generate.args.query\n",
    "        )  # 1st argument of retrieve_and_generate function\n",
    "        arg_retrieval_k_selector = (\n",
    "            Select.RecordCalls.retrieve_and_generate.args.k\n",
    "        )  # 2nd argument of retrieve_and_generate function\n",
    "\n",
    "        arg_completion_str_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            0\n",
    "        ]  # 1st returned value from retrieve_and_generate function\n",
    "        arg_retrieved_context_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            1\n",
    "        ]  # 2nd returned value from retrieve_and_generate function\n",
    "        arg_relevance_scores_selector = Select.RecordCalls.retrieve_and_generate.rets[\n",
    "            2\n",
    "        ]  # last returned value from retrieve_and_generate function\n",
    "\n",
    "        f_ir_hit_rate = (\n",
    "            Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).ir_hit_rate,\n",
    "                name=\"IR hit rate\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "\n",
    "        f_ndcg_at_k = (\n",
    "            Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).ndcg_at_k,\n",
    "                name=\"NDCG@k\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_relevance_scores_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "\n",
    "\n",
    "        f_recall_at_k = (\n",
    "                Feedback(\n",
    "                GroundTruthAgreement(self.ground_truth, provider=fOpenAI()).recall_at_k,\n",
    "                name=\"Recall@k\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_retrieved_context_selector)\n",
    "            .on(arg_relevance_scores_selector)\n",
    "            .on(arg_retrieval_k_selector)\n",
    "        )\n",
    "        f_groundtruth_answer = (\n",
    "            Feedback(\n",
    "            GroundTruthAgreement(self.ground_truth).agreement_measure,\n",
    "            name=\"Ground Truth answer (semantic similarity)\",\n",
    "            )\n",
    "            .on(arg_query_selector)\n",
    "            .on(arg_completion_str_selector))\n",
    "        return [f_ir_hit_rate, f_ndcg_at_k, f_recall_at_k, f_groundtruth_answer]\n",
    "\n",
    "    def _init_app(self):\n",
    "\n",
    "        tru_app = TruCustomApp(\n",
    "            self.rag_app,\n",
    "            app_name=self.name,\n",
    "            feedbacks=self.feedback,\n",
    "            )\n",
    "        return tru_app\n",
    "    def run(self ):\n",
    "        queries = self.ground_truth[\"query\"]\n",
    "        for i,query in enumerate(queries):\n",
    "            with self.tru_app as recording:\n",
    "                self.rag_app.retrieve_and_generate(query,10)\n",
    "    def leaderboard(self):\n",
    "        self.session.get_leaderboard(app_ids=[self.tru_app.app_id])\n",
    "\n",
    "\n",
    "\n",
    "class rag_app:\n",
    "    def __init__(self, retriever, generator, expected_responses,queries):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        self.expected_responses = expected_responses\n",
    "        self.queries = queries\n",
    "    \n",
    "    def _get_scores(self,chunks,expected_response):\n",
    "        chunks = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "        return [ score_chunk( chunk , expected_response)  for chunk in chunks]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @instrument\n",
    "    def retrieve_and_generate(self, query, k,):\n",
    "        chunks = self.retriever.get_Chunks(query)\n",
    "        chunks_dict = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "        response = self.generator.generate(query, chunks_dict)\n",
    "        i = self.queries.index(query)\n",
    "        expected_response = self.expected_responses[i]\n",
    "        scores = self._get_scores(chunks,expected_response)\n",
    "\n",
    "        return response, chunks_dict, scores\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In IR hit rate, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "✅ In IR hit rate, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "✅ In IR hit rate, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "✅ In NDCG@k, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "✅ In NDCG@k, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "✅ In NDCG@k, input relevance_scores will be set to __record__.app.retrieve_and_generate.rets[2] .\n",
      "✅ In NDCG@k, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "✅ In Recall@k, input query will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "✅ In Recall@k, input retrieved_context_chunks will be set to __record__.app.retrieve_and_generate.rets[1] .\n",
      "✅ In Recall@k, input relevance_scores will be set to __record__.app.retrieve_and_generate.rets[2] .\n",
      "✅ In Recall@k, input k will be set to __record__.app.retrieve_and_generate.args.k .\n",
      "✅ In Ground Truth answer (semantic similarity), input prompt will be set to __record__.app.retrieve_and_generate.args.query .\n",
      "✅ In Ground Truth answer (semantic similarity), input response will be set to __record__.app.retrieve_and_generate.rets[0] .\n"
     ]
    }
   ],
   "source": [
    "from cohere_ret.cohere_ret import cohere_retriever\n",
    "from cohere_ret.generator import cohere_generator\n",
    "from gemini.retrieve import gemini_retriever\n",
    "from openai_class.retriever import openai_retriever\n",
    "from openai_class.generator import openai_generator\n",
    "from voyageai_ret.retrieve import voyage_retriever\n",
    "from gemini.generator import gemini_generator\n",
    "from utils.prepare_ground_truth import LatestGroundTruthCSV\n",
    "from evaluator.ret_eval import rag_app, retriever_evaluator\n",
    "from utils.chunk_scorer import score_chunk\n",
    "csv_filepath = 'GroundTruths_Dataset - Sheet1.csv'\n",
    "json_filepath = 'URL-chunk_map.json'\n",
    "\n",
    "processor = LatestGroundTruthCSV(csv_filepath, json_filepath)\n",
    "ground_truth = processor.get_latest_ground_truth()\n",
    "\n",
    "\n",
    "\n",
    "ret = openai_retriever()\n",
    "gen = gemini_generator()\n",
    "rag_app = rag_app(ret, gen,ground_truth[\"expected_response\"],ground_truth[\"query\"])\n",
    "\n",
    "#eval-{Retriever}-{generator}-{chunksize}\n",
    "ret_eval = retriever_evaluator(name=\"eval_openai_gemini-528\",ground_truth=ground_truth,rag_app=rag_app, reset_db=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved and evaluated 0.0% \"How do I register for controlled or semi-controlled drugs custody?\"\n",
      "retrieved and evaluated 0.26109660574412535% \"What are the requirements for renewing the registration of a conventional pharmaceutical product?\"\n",
      "retrieved and evaluated 0.7874015748031497% \"How do I appeal a decision made by the Medical Licensing Committee?\"\n",
      "retrieved and evaluated 0.7712082262210797% \"What is the process for obtaining a certificate of amendment for registered pharmaceutical products?\"\n",
      "retrieved and evaluated 1.0204081632653061% \"How can I get a product classified?\"\n",
      "retrieved and evaluated 1.0% \"What are the steps to re-license a pharmaceutical facility?\"\n",
      "retrieved and evaluated 1.5384615384615385% \"How can I renew my license as a nurse or medical professional?\"\n",
      "retrieved and evaluated 2.1148036253776437% \"What's the process for getting a permit to import medical equipment?\"\n",
      "retrieved and evaluated 2.8268551236749118% \"How can I renew my health facility license?\"\n",
      "retrieved and evaluated 3.237410071942446% \"What are the steps to register a change in a doctor's professional title?\"\n",
      "retrieved and evaluated 3.90625% \"How do I re-license a health facility after cancellation or suspension?\"\n",
      "retrieved and evaluated 4.545454545454546% \"How do I change the technical director of my private medical facility?\"\n",
      "retrieved and evaluated 5.405405405405405% \"What's the process for re-licensing nurses and medical professionals?\"\n",
      "retrieved and evaluated 9.285714285714286% \"How can I request a list of licensed pharmaceutical facilities in the UAE?\"\n",
      "retrieved and evaluated 4.682274247491639% \"How do I renew my registration certificate to practice nursing or midwifery?\"\n",
      "retrieved and evaluated 2.332814930015552% \"What are the requirement documents for the good standing certificate of medical staff in the sector the is fee-exempt for renewal staff licenses?\"\n",
      "retrieved and evaluated 14.545454545454545% \" I have a medical equipment that is manufactured from animal-based products, What is the condition to renew the license? \"\n",
      "retrieved and evaluated 2.72% \"What are the fees to renew the document I get to open a clinic in the UAE? \"\n",
      "retrieved and evaluated 6.040268456375839% \"What are the required documents to apply for the approval of the service that enables employees to apply and approve their applications but for private entity employees? \"\n",
      "retrieved and evaluated 10.919540229885058% \"Do healthcare facilities need to fulfill specific requirements regarding elevator installations and what medical staff arrangements are required for operations?\"\n",
      "retrieved and evaluated 10.582010582010582% \"Can pharmaceutical companies export narcotic drugs and what are the validity requirements for such permits?\"\n",
      "retrieved and evaluated 8.01526717557252% \"Are there specific requirements for medical professionals over 60 years old and what documentation is needed for their continued practice?\"\n",
      "retrieved and evaluated 5.472636815920398% \"What restrictions apply to medical advertising on websites and social media, and how does the licensing differ between platforms?\"\n"
     ]
    }
   ],
   "source": [
    "ret_eval.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Ground Truth answer (semantic similarity)</th>\n",
       "      <th>IR hit rate</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_cohere_gemini-528</th>\n",
       "      <th>base</th>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355520</td>\n",
       "      <td>3.682347</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_openai_gemini-528</th>\n",
       "      <th>base</th>\n",
       "      <td>0.713636</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.289183</td>\n",
       "      <td>3.859978</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_gemini_gemini-528</th>\n",
       "      <th>base</th>\n",
       "      <td>0.708696</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.348931</td>\n",
       "      <td>6.009009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Ground Truth answer (semantic similarity)  \\\n",
       "app_name               app_version                                              \n",
       "eval_cohere_gemini-528 base                                          0.739130   \n",
       "eval_openai_gemini-528 base                                          0.713636   \n",
       "eval_gemini_gemini-528 base                                          0.708696   \n",
       "\n",
       "                                    IR hit rate  NDCG@k  Recall@k   latency  \\\n",
       "app_name               app_version                                            \n",
       "eval_cohere_gemini-528 base            0.913043     1.0  0.355520  3.682347   \n",
       "eval_openai_gemini-528 base            0.956522     1.0  0.289183  3.859978   \n",
       "eval_gemini_gemini-528 base            0.956522     1.0  0.348931  6.009009   \n",
       "\n",
       "                                    total_cost  \n",
       "app_name               app_version              \n",
       "eval_cohere_gemini-528 base                0.0  \n",
       "eval_openai_gemini-528 base                0.0  \n",
       "eval_gemini_gemini-528 base                0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_eval.session.get_leaderboard(app_ids=[ret_eval.tru_app.app_id,\"app_hash_ade3456fdd15fca7ae28c140723e81d8\",\"app_hash_9b4a142b2305ab226bfba93f80761a19\",\"app_hash_2a5e4537ce3ab13ec71ad41402a4978b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'app_hash_2a5e4537ce3ab13ec71ad41402a4978b'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_eval.tru_app.app_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
